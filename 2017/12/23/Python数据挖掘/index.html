<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-80426918-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-80426918-1');
</script>
<!-- End Google Analytics -->

  
  <title>Python 数据挖掘 | Hello Underground</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="AI 培训练习的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Python 数据挖掘">
<meta property="og:url" content="https://liuzesen.com/2017/12/23/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">
<meta property="og:site_name" content="Hello Underground">
<meta property="og:description" content="AI 培训练习的笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2017-12-23T09:19:18.000Z">
<meta property="article:modified_time" content="2017-12-23T09:19:18.000Z">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@hnliuzesen">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hello Underground</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://liuzesen.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Python数据挖掘" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/12/23/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" class="article-date">
  <time class="dt-published" datetime="2017-12-23T09:19:18.000Z" itemprop="datePublished">2017-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Develop/">Develop</a>►<a class="article-category-link" href="/categories/Develop/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Python 数据挖掘
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><em><a href="http://uml.org.cn/" target="_blank" rel="noopener">AI 培训</a>练习的笔记</em></p>
<span id="more"></span>

<h2 id="下载数据"><a href="#下载数据" class="headerlink" title="下载数据"></a>下载数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://archive.ics.uci.edu/ml/datasets/Iris</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;iris.csv&#x27;</span>):</span><br><span class="line">    urlString = <span class="string">&#x27;http://aima.cs.berkeley.edu/data/iris.csv&#x27;</span></span><br><span class="line">    irisFile = urllib.request.urlopen(urlString)</span><br><span class="line">    localIrisFile = <span class="built_in">open</span>(<span class="string">&#x27;iris.csv&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    localIrisFile.write(irisFile.read())</span><br><span class="line">    localIrisFile.close()</span><br></pre></td></tr></table></figure>

<p>我们使用了 urllib 类库获取伯克利大学网站的一个数据文件，并使把它保存到本地磁盘。</p>
<p>数据包含鸢尾花（iris）数据集，这是一个包含了三种鸢尾花（山鸢尾 setosa、维吉尼亚鸢尾 virginica 和变色鸢尾 versicolor）的各 50 
个数据样本的多元数据集。</p>
<p>每个样本都有四个特征（或者说变量），即花萼（sepal）和花瓣（petal）的长度和宽度，单位为厘米。</p>
<p>数据集以 CSV（逗号分割值）的格式存储。CSV 文件可以很方便的转化并把其中的信息存储为适合的数据结构。此数据集有 5 列。格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5.0,3.3,1.4,0.2,setosa</span><br><span class="line">7.0,3.2,4.7,1.4,versicolor</span><br><span class="line">7.1,3.0,5.9,2.1,virginica</span><br></pre></td></tr></table></figure>

<h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取前四列特征 花萼（sepal）的长度和宽度花瓣（petal）的长度和宽度</span></span><br><span class="line">data = genfromtxt(<span class="string">&#x27;iris.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, usecols=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># 读取最后一列标注</span></span><br><span class="line">target = genfromtxt(<span class="string">&#x27;iris.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, usecols=(<span class="number">4</span>,), dtype=<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"><span class="comment"># (150, 4)</span></span><br><span class="line"><span class="built_in">print</span>(target.shape)</span><br><span class="line"><span class="comment"># (150,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">set</span>(target))</span><br><span class="line"><span class="comment"># &#123;&#x27;versicolor&#x27;, &#x27;virginica&#x27;, &#x27;setosa&#x27;&#125;</span></span><br></pre></td></tr></table></figure>

<p>上面的代码中创建了一个包含特征值的矩阵和一个包含样本类型的向量。我们可以通过查看我们加载的数据结构的 shape 
来确认数据集的大小，也可以查看我们有多少种样本类型以及它们的名字。</p>
<p>当我们处理新数据的时候，一项很重要的任务是尝试去理解数据包含的信息以及它的组织结构。可视化可以灵活生动的展示数据，帮助我们深入理解数据。</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> plot, show</span><br><span class="line"></span><br><span class="line">plot(data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;r+&#x27;</span>)</span><br><span class="line">plot(data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;g*&#x27;</span>)</span><br><span class="line">show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> figure, subplot, hist, xlim</span><br><span class="line"></span><br><span class="line">xmin = <span class="built_in">min</span>(data[:, <span class="number">0</span>])</span><br><span class="line">xmax = <span class="built_in">max</span>(data[:, <span class="number">0</span>])</span><br><span class="line">figure()  <span class="comment"># 新图层</span></span><br><span class="line">subplot(<span class="number">411</span>)  <span class="comment"># 图层分区</span></span><br><span class="line">hist(data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, alpha=<span class="number">.7</span>)  <span class="comment"># 绘制直方图</span></span><br><span class="line">xlim(xmin, xmax)  <span class="comment"># 设置坐标系</span></span><br><span class="line">subplot(<span class="number">412</span>)</span><br><span class="line">hist(data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line">subplot(<span class="number">413</span>)</span><br><span class="line">hist(data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line">subplot(<span class="number">414</span>)</span><br><span class="line">hist(data[:, <span class="number">0</span>], color=<span class="string">&#x27;y&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>

<p>图 1 中有 150 个点，不同的颜色代表不同的类型；蓝色点代表山鸢尾，红色点代表变色鸢尾，绿色点代表维吉尼亚鸢尾。</p>
<p>另一种常用的查看数据的方法是分特性绘制直方图。在图 2 
中，数据被分为三类，我们可以比较每一类的分布特征。第二段代码绘制数据中每一类型的第一个特性（花萼的长度）</p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> zeros</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分类用数字代替</span></span><br><span class="line">t = zeros(<span class="built_in">len</span>(target))</span><br><span class="line">t[target == <span class="string">&#x27;setosa&#x27;</span>] = <span class="number">1</span></span><br><span class="line">t[target == <span class="string">&#x27;versicolor&#x27;</span>] = <span class="number">2</span></span><br><span class="line">t[target == <span class="string">&#x27;virginica&#x27;</span>] = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用连续型朴素贝叶斯分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"></span><br><span class="line">classifier = GaussianNB()</span><br><span class="line">classifier.fit(data, t)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测：&quot;</span>, (classifier.predict([data[<span class="number">0</span>]])))</span><br><span class="line"><span class="comment"># 预测： [ 1.]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;结果：&quot;</span>, t[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果： 1.0</span></span><br></pre></td></tr></table></figure>

<p>分类是一个数据挖掘方法，用于把一个数据集中的样本数据分配给各个目标类。实现这个方法的模块叫做分类器。使用分类器需要以下两步：训练和分类。训练是指采集已知其特定类归属的数据并基于这些数据创建分类器。 分类是指使用通过这些已知数据建立的分类器来处理未知的数据，以判断未知数据的分类情况。</p>
<p>Sklearn 类库包含很多分类器的实现，第一段代码使用高斯朴素贝叶斯来分析我们在最开始载入的鸢尾花数据，包含山鸢尾、变色鸢尾和维吉尼亚鸢尾。最后把字符串数组转型成整型数据</p>
<p>分类器可以由 predict 方法完成，并且只要输出一个样例就可以很简单的检测。第二段代码中 predicted 
类包含了一个正确的样本（山鸢尾），但是在广泛的样本上评估分类器并且使用非训练环节的数据测试是很重要的。最终我们通过从源数据集中随机抽取样本把数据分为训练集和测试集。我们将会使用训练集的数据来训练分类器，并使用测试集的数据来测试分类器。train_test_split 方法正是实现此功能的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line">train, test, t_train, t_test = model_selection.train_test_split(data, t, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">classifier.fit(train, t_train)  <span class="comment"># 训练</span></span><br><span class="line"><span class="built_in">print</span>(classifier.score(test, t_test))  <span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 0.933333333333</span></span><br></pre></td></tr></table></figure>

<p>数据集被分一分为二，测试集被指定为源数据的40%（命名为 test_size），我们用它反复训练我们的分类器并输出精确度。</p>
<p>在此例中，我们的精确度为 93%。一个分类器的精确度是通过正确分类样本的数量除以总样本的数量得出的。也就是说，它意味着我们正确预测的比例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(classifier.predict(test), t_test))</span><br><span class="line"><span class="comment"># [[16  0  0]</span></span><br><span class="line"><span class="comment">#  [ 0 23  4]</span></span><br><span class="line"><span class="comment">#  [ 0  0 17]]</span></span><br></pre></td></tr></table></figure>

<p>另一个估计分类器表现的工具叫做混淆矩阵。在此矩阵中每列代表一个预测类的实例，每行代表一个实际类的实例。使用它可以很容易的计算和打印矩阵</p>
<p>在这个混淆矩阵中我们可以看到所有山鸢尾和维吉尼亚鸢尾都被正确的分类了，但是实际上应该是 26 
个的变色鸢尾，系统却预测其中三个是维吉尼亚鸢尾。如果我们牢记所有正确的猜测都在表格的对角线上，那么观测表格的错误就很容易了，即对角线以外的非零值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(classifier.predict(test), t_test, target_names=[<span class="string">&#x27;setosa&#x27;</span>, <span class="string">&#x27;versicolor&#x27;</span>, <span class="string">&#x27;virginica&#x27;</span>]))</span><br><span class="line"><span class="comment">#              precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      setosa       1.00      1.00      1.00        16</span></span><br><span class="line"><span class="comment">#  versicolor       1.00      0.85      0.92        27</span></span><br><span class="line"><span class="comment">#   virginica       0.81      1.00      0.89        17</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># avg / total       0.95      0.93      0.93        60</span></span><br></pre></td></tr></table></figure>

<p>Precision：正确预测的比例，属于该类的被准确分类为该类的比例。100% 也有可能包含别的样本被错误的归类到本类目下。</p>
<p>Recall（或者叫真阳性率）：正确识别的比例，被划分到此类下面的是否确实属于此类。100% 也有可能漏掉属于该类却没有划分到该类的样本。</p>
<p>F1-Score：precision和recall的调和平均数</p>
<p>以上仅仅只是给出用于支撑测试分类的数据量。当然，分割数据、减少用于训练的样本数以及评估结果等操作都依赖于配对的训练集和测试集的随机选择。如果要切实评估一个分类器并与其它的分类器作比较的话，我们需要使用一个更加精确的评估模型，例如 Cross Validation。该模型背后的思想很简单：多次将数据分为不同的训练集和测试集，最终分类器评估选取多次预测的平均值。这次，sklearn 为我们提供了运行模型的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">scores = cross_val_score(classifier, data, t, cv=<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(scores)</span><br><span class="line"><span class="comment"># [ 0.92592593  1.          0.91666667  0.91666667  0.95833333  1.        ]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mean</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mean(scores))</span><br><span class="line"><span class="comment"># 0.952932098765</span></span><br></pre></td></tr></table></figure>

<p>如上所见，输出是每次模型迭代产生的精确度的数组。我们可以很容易计算出平均精确度</p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>通常我们的数据上不会有标签告诉我们它的样本类型；我们需要分析数据，把数据按照它们的相似度标准分成不同的群组，群组（或者群集）指的是相似样本的集合。这种分析被称为无监督数据分析。最著名的聚类工具之一叫做 k-means 算法，如下所示：</p>
<p>划分:K-means,k-MEDOIDs</p>
<p>层次聚类：CURE，BIRCH</p>
<p>密度聚类：DBSCAN</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, init=<span class="string">&#x27;random&#x27;</span>)</span><br><span class="line">c = kmeans.fit_predict(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> completeness_score, homogeneity_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;完整性得分&#x27;</span>, completeness_score(t, c))</span><br><span class="line"><span class="comment"># 完整性得分 0.764986151449</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;同质性得分&#x27;</span>, homogeneity_score(t, c))</span><br><span class="line"><span class="comment"># 同质性得分 0.751485402199</span></span><br></pre></td></tr></table></figure>

<p>上述片段运行 k-measn 算法并把数据分为三个群集（参数 k 所指定的）。现在我们可以使用模型把每一个样本分配到三个群集中。我们可以估计群集的结果，与使用完整性得分和同质性得分计算而得的标签作比较</p>
<p>同质性 homogeneity：每个群集只包含单个类的成员；</p>
<p>完整性 completeness：给定类的所有成员都分配给同一个群集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">figure()</span><br><span class="line">subplot(<span class="number">211</span>)</span><br><span class="line">plot(data[t == <span class="number">1</span>, <span class="number">0</span>], data[t == <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(data[t == <span class="number">2</span>, <span class="number">0</span>], data[t == <span class="number">2</span>, <span class="number">2</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">plot(data[t == <span class="number">3</span>, <span class="number">0</span>], data[t == <span class="number">3</span>, <span class="number">2</span>], <span class="string">&#x27;go&#x27;</span>)</span><br><span class="line">subplot(<span class="number">212</span>)</span><br><span class="line">plot(data[c == <span class="number">0</span>, <span class="number">0</span>], data[c == <span class="number">0</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c == <span class="number">1</span>, <span class="number">0</span>], data[c == <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;ro&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c == <span class="number">2</span>, <span class="number">0</span>], data[c == <span class="number">2</span>, <span class="number">2</span>], <span class="string">&#x27;go&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>

<p>我们可以集群可视化并和带有真实标签的做可视化比较。</p>
<p>观察此图我们可以看到，底部左侧的群集可以被 k-means 完全识别，然而顶部的两个群集有部分识别错误。</p>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>回归是一个用于预测变量之间函数关系的方法。例如，我们有两个变量，一个被认为是解释，一个被认为是依赖。我们希望使用模型描述两者的关系。当这种关系是一条线的时候就称为线性回归。</p>
<p>为了应用线性回归我们建立一个由上所述的综合数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> rand</span><br><span class="line"></span><br><span class="line">x = rand(<span class="number">40</span>, <span class="number">1</span>)  <span class="comment"># 解释变量</span></span><br><span class="line">y = x * x * x + rand(<span class="number">40</span>, <span class="number">1</span>) / <span class="number">5</span>  <span class="comment"># 依赖变量</span></span><br></pre></td></tr></table></figure>

<p>使用在 sklear.linear_model 模块中的 LinearRegression 
模型。该模型可以通过计算每个数据点到拟合线的垂直差的平方和，找到平方和最小的最佳拟合线。使用方法和我们之前遇到的实现 sklearn 
的模型类似。然后通过把拟合线和实际数据点画在同一幅图上来评估结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linspace, matrix</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">xx = linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">40</span>)</span><br><span class="line">plot(x, y, <span class="string">&#x27;o&#x27;</span>, xx, linreg.predict(matrix(xx).T), <span class="string">&#x27;--r&#x27;</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>

<p>观察该图我们可以得出结论：拟合线从数据点中心穿过，并可以确定是增长的趋势。</p>
<p>使用均方误差来量化模型和原始数据的拟合度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;拟合度：&#x27;</span>, mean_squared_error(linreg.predict(x), y))</span><br><span class="line"><span class="comment"># 拟合度： 0.0175373207578</span></span><br></pre></td></tr></table></figure>

<h2 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h2><p>我们通过研究相关性来理解成对的变量之间是否相关，相关性的强弱。此类分析帮助我们精确定位被依赖的重要变量。最好的相关方法是皮尔逊积矩相关系数。它是由两个变量的协方差除以它们的标准差的乘积计算而来。我们将鸢尾花数据集的变量两两组合计算出其系数如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> corrcoef</span><br><span class="line"></span><br><span class="line">corr = corrcoef(data.T)  <span class="comment"># .T gives the transpose</span></span><br><span class="line"><span class="built_in">print</span>(corr)</span><br><span class="line"><span class="comment"># [[ 1.         -0.10936925  0.87175416  0.81795363]</span></span><br><span class="line"><span class="comment">#  [-0.10936925  1.         -0.4205161  -0.35654409]</span></span><br><span class="line"><span class="comment">#  [ 0.87175416 -0.4205161   1.          0.9627571 ]</span></span><br><span class="line"><span class="comment">#  [ 0.81795363 -0.35654409  0.9627571   1.        ]]</span></span><br></pre></td></tr></table></figure>

<p>corrcoef 方法通过输入行为变量列为观察值的矩阵，计算返回相关系数的对称矩阵。该矩阵的每个元素代表着两个变量的相关性。当值一起增长时相关性为正。当一个值减少而另一个只增加时相关性为负。特别说明，1 代表完美的正相关，0 代表不相关，-1 代表完美的负相关。</p>
<p>当变量数增长时我们可以使用伪彩色点很方便的可视化相关矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> pcolor, colorbar, xticks, yticks</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> arange</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">pcolor(corr)</span><br><span class="line">colorbar()  <span class="comment"># add</span></span><br><span class="line"><span class="comment"># 在坐标系上标识名称</span></span><br><span class="line">xticks(arange(<span class="number">0.5</span>, <span class="number">4.5</span>), [<span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal length&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span>], rotation=-<span class="number">20</span>)</span><br><span class="line">yticks(arange(<span class="number">0.5</span>, <span class="number">4.5</span>), [<span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal length&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span>], rotation=-<span class="number">20</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>

<p>看图右侧的彩条，我们可以把颜色点关联到数值上。在本例中，红色被关联为最高的正相关，我们可以看出我们数据集的最强相关是“花瓣宽度”和“花瓣长度”这两个变量。</p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>单独使用该方法，我们只能看到数据集的部分数据视图。既然我们可以同时绘制的最高维度数为3，将整个数据集嵌入一系列维度并建立一个整体可视化视图是很有必要的。这个嵌入过程就被称作降维。最著名的降维技术之一就是主成分分析（PCA）。该技术把数据变量转换为等量或更少的不相关变量，称为主成分（PCs）。</p>
<p>们实例化了一个PCA对象，用于计算前两个主成分。然后如往常一样绘制结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pcad = pca.fit_transform(data)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;go&#x27;</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>

<p>可以注意到上图和第一章提到的有些相似，不过这次变色鸢尾（红色的）和维吉尼亚鸢尾（绿色的）的间隔更清晰了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(pca.explained_variance_ratio_)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span> - <span class="built_in">sum</span>(pca.explained_variance_ratio_))</span><br><span class="line">data_inv = pca.inverse_transform(pcad)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">abs</span>(<span class="built_in">sum</span>(<span class="built_in">sum</span>(data - data_inv))))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">    pca = PCA(n_components=i)</span><br><span class="line">    pca.fit(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">sum</span>(pca.explained_variance_ratio_) * <span class="number">100</span>, <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>PCA将空间数据方差最大化，我们可以通过方差比判断 PCs 包含的信息量：pca.explained_variance_ratio_</p>
<p>现在我们知道第一个PC占原始数据的92%的信息量而第二个占剩下的5%。我们还可以输出在转化过程中丢失的信息量：pca.explained_variance_ratio_</p>
<p>在本例中我们损失了2%的信息量。此时，我们可以是应用逆变换还原原始数据：pca.inverse_transform</p>
<p>可以证明的是，由于信息丢失逆变换不能给出准确的原始数据。我们可以估算逆变换的结果和原始数据的相似度：abs(sum(sum(data - data_inv)))</p>
<p>可以看出原始数据和逆变换计算出的近似值之间的差异接近于零。通过改变主成分的数值来计算我们能够覆盖多少信息量是很有趣的。PCs 
用得越多，信息覆盖就越全，不过这段分析有助于我们理解保存一段特定的信息需要哪些组件。例如，从上述片段可以看出，只要使用三个 PCs 
就可以覆盖鸢尾花数据集的几乎 100% 的信息。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载数据</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://archive.ics.uci.edu/ml/datasets/Iris</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;iris.csv&#x27;</span>):</span><br><span class="line">    urlString = <span class="string">&#x27;http://aima.cs.berkeley.edu/data/iris.csv&#x27;</span></span><br><span class="line">    irisFile = urllib.request.urlopen(urlString)</span><br><span class="line">    localIrisFile = <span class="built_in">open</span>(<span class="string">&#x27;iris.csv&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    localIrisFile.write(irisFile.read())</span><br><span class="line">    localIrisFile.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据导入</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取前四列特征 花萼（sepal）的长度和宽度花瓣（petal）的长度和宽度</span></span><br><span class="line">data = genfromtxt(<span class="string">&#x27;iris.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, usecols=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># 读取最后一列标注</span></span><br><span class="line">target = genfromtxt(<span class="string">&#x27;iris.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, usecols=(<span class="number">4</span>,), dtype=<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"><span class="comment"># (150, 4)</span></span><br><span class="line"><span class="built_in">print</span>(target.shape)</span><br><span class="line"><span class="comment"># (150,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">set</span>(target))</span><br><span class="line"><span class="comment"># &#123;&#x27;versicolor&#x27;, &#x27;virginica&#x27;, &#x27;setosa&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> plot, show</span><br><span class="line"></span><br><span class="line">plot(data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;r+&#x27;</span>)</span><br><span class="line">plot(data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">2</span>], <span class="string">&#x27;g*&#x27;</span>)</span><br><span class="line"><span class="comment"># show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> figure, subplot, hist, xlim</span><br><span class="line"></span><br><span class="line">xmin = <span class="built_in">min</span>(data[:, <span class="number">0</span>])</span><br><span class="line">xmax = <span class="built_in">max</span>(data[:, <span class="number">0</span>])</span><br><span class="line">figure()  <span class="comment"># 新图层</span></span><br><span class="line">subplot(<span class="number">411</span>)  <span class="comment"># 图层分区</span></span><br><span class="line">hist(data[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, alpha=<span class="number">.7</span>)  <span class="comment"># 绘制直方图</span></span><br><span class="line">xlim(xmin, xmax)  <span class="comment"># 设置坐标系</span></span><br><span class="line">subplot(<span class="number">412</span>)</span><br><span class="line">hist(data[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line">subplot(<span class="number">413</span>)</span><br><span class="line">hist(data[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line">subplot(<span class="number">414</span>)</span><br><span class="line">hist(data[:, <span class="number">0</span>], color=<span class="string">&#x27;y&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin, xmax)</span><br><span class="line"><span class="comment"># show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> zeros</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分类用数字代替</span></span><br><span class="line">t = zeros(<span class="built_in">len</span>(target))</span><br><span class="line">t[target == <span class="string">&#x27;setosa&#x27;</span>] = <span class="number">1</span></span><br><span class="line">t[target == <span class="string">&#x27;versicolor&#x27;</span>] = <span class="number">2</span></span><br><span class="line">t[target == <span class="string">&#x27;virginica&#x27;</span>] = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用连续型朴素贝叶斯分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"></span><br><span class="line">classifier = GaussianNB()</span><br><span class="line">classifier.fit(data, t)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测：&#x27;</span>, (classifier.predict([data[<span class="number">0</span>]])))</span><br><span class="line"><span class="comment"># 预测： [ 1.]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;结果：&#x27;</span>, t[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果： 1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line">train, test, t_train, t_test = model_selection.train_test_split(data, t, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">classifier.fit(train, t_train)  <span class="comment"># 训练</span></span><br><span class="line"><span class="built_in">print</span>(classifier.score(test, t_test))  <span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 0.933333333333</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(classifier.predict(test), t_test))</span><br><span class="line"><span class="comment"># [[16  0  0]</span></span><br><span class="line"><span class="comment">#  [ 0 23  4]</span></span><br><span class="line"><span class="comment">#  [ 0  0 17]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(classifier.predict(test), t_test, target_names=[<span class="string">&#x27;setosa&#x27;</span>, <span class="string">&#x27;versicolor&#x27;</span>, <span class="string">&#x27;virginica&#x27;</span>]))</span><br><span class="line"><span class="comment">#              precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      setosa       1.00      1.00      1.00        16</span></span><br><span class="line"><span class="comment">#  versicolor       1.00      0.85      0.92        27</span></span><br><span class="line"><span class="comment">#   virginica       0.81      1.00      0.89        17</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># avg / total       0.95      0.93      0.93        60</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">scores = cross_val_score(classifier, data, t, cv=<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(scores)</span><br><span class="line"><span class="comment"># [ 0.92592593  1.          0.91666667  0.91666667  0.95833333  1.        ]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mean</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平均精确度&#x27;</span>, mean(scores))</span><br><span class="line"><span class="comment"># 平均精确度 0.952932098765</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, init=<span class="string">&#x27;random&#x27;</span>)</span><br><span class="line">c = kmeans.fit_predict(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> completeness_score, homogeneity_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;完整性得分&#x27;</span>, completeness_score(t, c))</span><br><span class="line"><span class="comment"># 完整性得分 0.764986151449</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;同质性得分&#x27;</span>, homogeneity_score(t, c))</span><br><span class="line"><span class="comment"># 同质性得分 0.751485402199</span></span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">subplot(<span class="number">211</span>)</span><br><span class="line">plot(data[t == <span class="number">1</span>, <span class="number">0</span>], data[t == <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(data[t == <span class="number">2</span>, <span class="number">0</span>], data[t == <span class="number">2</span>, <span class="number">2</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">plot(data[t == <span class="number">3</span>, <span class="number">0</span>], data[t == <span class="number">3</span>, <span class="number">2</span>], <span class="string">&#x27;go&#x27;</span>)</span><br><span class="line">subplot(<span class="number">212</span>)</span><br><span class="line">plot(data[c == <span class="number">0</span>, <span class="number">0</span>], data[c == <span class="number">0</span>, <span class="number">2</span>], <span class="string">&#x27;bo&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c == <span class="number">1</span>, <span class="number">0</span>], data[c == <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;ro&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c == <span class="number">2</span>, <span class="number">0</span>], data[c == <span class="number">2</span>, <span class="number">2</span>], <span class="string">&#x27;go&#x27;</span>, alpha=<span class="number">.7</span>)</span><br><span class="line"><span class="comment"># show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回归</span></span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> rand</span><br><span class="line"></span><br><span class="line">x = rand(<span class="number">40</span>, <span class="number">1</span>)  <span class="comment"># 解释变量</span></span><br><span class="line">y = x * x * x + rand(<span class="number">40</span>, <span class="number">1</span>) / <span class="number">5</span>  <span class="comment"># 依赖变量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linspace, matrix</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">xx = linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">40</span>)</span><br><span class="line">plot(x, y, <span class="string">&#x27;o&#x27;</span>, xx, linreg.predict(matrix(xx).T), <span class="string">&#x27;--r&#x27;</span>)</span><br><span class="line">show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;拟合度：&#x27;</span>, mean_squared_error(linreg.predict(x), y))</span><br><span class="line"><span class="comment"># 拟合度： 0.0175373207578</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> corrcoef</span><br><span class="line"></span><br><span class="line">corr = corrcoef(data.T)  <span class="comment"># .T gives the transpose</span></span><br><span class="line"><span class="built_in">print</span>(corr)</span><br><span class="line"><span class="comment"># [[ 1.         -0.10936925  0.87175416  0.81795363]</span></span><br><span class="line"><span class="comment">#  [-0.10936925  1.         -0.4205161  -0.35654409]</span></span><br><span class="line"><span class="comment">#  [ 0.87175416 -0.4205161   1.          0.9627571 ]</span></span><br><span class="line"><span class="comment">#  [ 0.81795363 -0.35654409  0.9627571   1.        ]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> pcolor, colorbar, xticks, yticks</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> arange</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">pcolor(corr)</span><br><span class="line">colorbar()  <span class="comment"># add</span></span><br><span class="line"><span class="comment"># 在坐标系上标识名称</span></span><br><span class="line">xticks(arange(<span class="number">0.5</span>, <span class="number">4.5</span>), [<span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal length&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span>], rotation=-<span class="number">20</span>)</span><br><span class="line">yticks(arange(<span class="number">0.5</span>, <span class="number">4.5</span>), [<span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal length&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span>], rotation=-<span class="number">20</span>)</span><br><span class="line"><span class="comment"># show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 降维</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">figure()</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pcad = pca.fit_transform(data)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;setosa&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;versicolor&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">plot(pcad[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">0</span>], pcad[target == <span class="string">&#x27;virginica&#x27;</span>, <span class="number">1</span>], <span class="string">&#x27;go&#x27;</span>)</span><br><span class="line">show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pca.explained_variance_ratio_)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span> - <span class="built_in">sum</span>(pca.explained_variance_ratio_))</span><br><span class="line">data_inv = pca.inverse_transform(pcad)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">abs</span>(<span class="built_in">sum</span>(<span class="built_in">sum</span>(data - data_inv))))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">    pca = PCA(n_components=i)</span><br><span class="line">    pca.fit(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">sum</span>(pca.explained_variance_ratio_) * <span class="number">100</span>, <span class="string">&#x27;%&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://liuzesen.com/2017/12/23/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" data-id="ckr5sx43e003detop2s5f8s0v" data-title="Python 数据挖掘" class="article-share-link">分享</a>
      
      
      
      
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
var gitalk = new Gitalk({
  clientID: 'b6df7cac6bd14222537c',
  clientSecret: '8ac65aa69a0230023632dc095585e73cd72476dd',
  repo: 'hnliuzesen.github.com',
  owner: 'hnliuzesen',
  admin: 'hnliuzesen',
  id: decodeURI(window.location.pathname).split("/")[4],
  distractionFreeMode: 'true'
})
gitalk.render('gitalk-container')
</script>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/05/06/%E8%B7%AF%E7%94%B1%E5%99%A8%E8%87%AA%E5%8A%A8%E7%BB%91%E5%AE%9AIP%E5%88%B0CloudFlare/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          路由器自动绑定 IP 到 CloudFlare
        
      </div>
    </a>
  
  
    <a href="/2017/12/12/convert-azw3-to-mobi/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">convert azw3 to mobi</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/">Computers & Technology</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/Git/">Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Computers-Technology/Program/">Program</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Develop/">Develop</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Develop/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Develop/Machine-Learning/">Machine Learning</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tip/">Tip</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Tip/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tip/Kindle/">Kindle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tip/WEB/">WEB</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Android/" style="font-size: 13.33px;">Android</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Binary-Search/" style="font-size: 10px;">Binary Search</a> <a href="/tags/CloudFlare/" style="font-size: 10px;">CloudFlare</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 10px;">Divide and Conquer</a> <a href="/tags/Embedded/" style="font-size: 16.67px;">Embedded</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Google-Play/" style="font-size: 10px;">Google Play</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/Java/" style="font-size: 16.67px;">Java</a> <a href="/tags/Kindle/" style="font-size: 10px;">Kindle</a> <a href="/tags/Lombok/" style="font-size: 10px;">Lombok</a> <a href="/tags/MIUI/" style="font-size: 10px;">MIUI</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/PostgreSQL/" style="font-size: 10px;">PostgreSQL</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/SourceTree/" style="font-size: 10px;">SourceTree</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/String/" style="font-size: 13.33px;">String</a> <a href="/tags/TensorFlow/" style="font-size: 10px;">TensorFlow</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/azw3/" style="font-size: 10px;">azw3</a> <a href="/tags/domain/" style="font-size: 10px;">domain</a> <a href="/tags/mobi/" style="font-size: 10px;">mobi</a> <a href="/tags/router/" style="font-size: 10px;">router</a>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Hello Underground<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>







  </div>
</body>
</html>